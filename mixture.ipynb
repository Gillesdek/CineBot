{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "class SimilarityRecommenderService:\n",
    "    def __init__(self):\n",
    "        self._cuda_enabled = True if os.getenv(\"CUDA_ENABLED\") == 1 else False\n",
    "        cache_dir = os.getenv(\"CACHE_DIR\")\n",
    "        self._cache_dir = cache_dir if cache_dir else \".cache_dir\"\n",
    "        self.model = SentenceTransformer(\n",
    "            \"paraphrase-MiniLM-L6-v2\",\n",
    "            device=None if not self._cuda_enabled else \"cuda\",\n",
    "            cache_folder=self._cache_dir,\n",
    "        )\n",
    "        self._embedded_corpus = list()\n",
    "        self._corpus_mapped_data = list()  # recommended movie\n",
    "        self._corpus = list()  # Corpus of\n",
    "\n",
    "    def _train_from_jsonl(self, file_path=None):\n",
    "        if not file_path:\n",
    "            file_path = \"Data/train_data.jsonl\"\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                # Load each line as JSON\n",
    "                json_data = json.loads(line.strip())\n",
    "                joined_text = \" \".join([d[\"text\"] for d in json_data[\"messages\"]])\n",
    "                self._corpus.append(joined_text)\n",
    "                self._corpus_mapped_data.append(json_data[\"movieMentions\"])\n",
    "        self._embedded_corpus = self.model.encode(self._corpus, convert_to_tensor=True)\n",
    "\n",
    "    def _first_run_setup(self):\n",
    "        self._train_from_jsonl()\n",
    "        with open(f\"{self._cache_dir}/corpus_embedded.pkl\", \"wb\") as f0, open(\n",
    "            f\"{self._cache_dir}/corpus_mapped_data.pkl\", \"wb\"\n",
    "        ) as f1:\n",
    "            pickle.dump(self._embedded_corpus, f0)\n",
    "            pickle.dump((self._corpus, self._corpus_mapped_data), f1)\n",
    "\n",
    "    def init(self, *args, **kwargs):\n",
    "        if not os.path.exists(f\"{self._cache_dir}/.initialized\"):\n",
    "            self._first_run_setup()\n",
    "            with open(f\"{self._cache_dir}/.initialized\", mode=\"a\"):\n",
    "                pass\n",
    "        with open(f\"{self._cache_dir}/corpus_embedded.pkl\", \"rb\") as f0, open(\n",
    "            f\"{self._cache_dir}/corpus_mapped_data.pkl\", \"rb\"\n",
    "        ) as f1:\n",
    "            self._embedded_corpus = pickle.load(f0)\n",
    "            self._corpus, self._corpus_mapped_data = pickle.load(f1)\n",
    "        if self._cuda_enabled:\n",
    "            self._embedded_corpus = util.normalize_embeddings(\n",
    "                self._embedded_corpus.to(\"cuda\")\n",
    "            )\n",
    "\n",
    "    def _retrieve_top_k_similar_queries(self, query, k=5):\n",
    "        embeded_query = self.model.encode(query, convert_to_tensor=True)\n",
    "        similarities = []\n",
    "        for i in range(len(self._corpus)):\n",
    "            sim = util.cos_sim(\n",
    "                embeded_query.reshape(1, 384), self._corpus[i].reshape(1, 384)\n",
    "            )\n",
    "            similarities.append((sim, i))\n",
    "        similarities.sort(reverse=True)  # Sort in descending order of similarity\n",
    "\n",
    "        top_k_similar_queries = []\n",
    "        for sim, idx in similarities[:k]:\n",
    "            query, movie_mentions = self.query_movie_map[idx]\n",
    "            top_k_similar_queries.append((idx, query, movie_mentions, sim))\n",
    "        return top_k_similar_queries\n",
    "\n",
    "    def simi_top_k(self, query, k=5):\n",
    "        query_embeddings = self.model.encode(query, convert_to_tensor=True)\n",
    "        if self._cuda_enabled:\n",
    "            query_embeddings = query_embeddings.to(\"cuda\")\n",
    "            try:\n",
    "                query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return util.semantic_search(\n",
    "            query_embeddings,\n",
    "            self._embedded_corpus,\n",
    "            score_function=util.dot_score,\n",
    "            top_k=k,\n",
    "        )[0]\n",
    "\n",
    "    def recommend(self, query, k=3):\n",
    "        similar_queries = self.simi_top_k(query, k=5)\n",
    "        movie_mentions_list = []\n",
    "        for similar_query in similar_queries:\n",
    "            movie_mentions = self._corpus_mapped_data[similar_query[\"corpus_id\"]]\n",
    "            if movie_mentions:\n",
    "                movie_mentions_list.extend(movie_mentions.values())\n",
    "\n",
    "        # Count occurrences of each movie\n",
    "        movie_mentions_counter = Counter(movie_mentions_list)\n",
    "        # Find the top k most mentioned movies\n",
    "        movie_to_count_map = movie_mentions_counter.most_common(k)\n",
    "        # Extract movie titles from the top k movies\n",
    "        # recommended_movies = [movie for movie, nb_ in movie_to_count_map]\n",
    "        retval = list(zip(*movie_to_count_map))\n",
    "        return retval[0], retval[1]\n",
    "\n",
    "    def _deprecated_recommend(self, query, k=1):\n",
    "        similar_queries = self._retrieve_top_k_similar_queries(query)\n",
    "        movie_mentions_list = []\n",
    "        for similar_query in similar_queries:\n",
    "            movie_mentions = similar_query[2]\n",
    "            movie_mentions_list.extend(list(movie_mentions.values()))\n",
    "\n",
    "        # Count occurrences of each movie\n",
    "        movie_mentions_counter = Counter(movie_mentions_list)\n",
    "\n",
    "        # Find the top k most mentioned movies\n",
    "        top_k_movies = movie_mentions_counter.most_common(k)\n",
    "\n",
    "        # Extract movie titles from the top k movies\n",
    "        recommended_movies = [movie for movie, _ in top_k_movies]\n",
    "\n",
    "        return recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shelt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shelt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "##Â load data\n",
    "df = pd.read_csv(\"Data/IMDB_top_1000.csv\")\n",
    "plots = df[\"Overview\"].values.tolist()\n",
    "for i in range(len(plots)):\n",
    "    plots[\n",
    "        i\n",
    "    ] = f\"{plots[i]}  {df.loc[i,'Genre']}  {df.loc[i,'Director']} {df.loc[i,'Star1']} {df.loc[i,'Star2']} {df.loc[i,'Star3']} {df.loc[i,'Star4']}\"\n",
    "\n",
    "# Preprocessing\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and stopwords, and apply stemming\n",
    "    processed_tokens = [\n",
    "        stemmer.stem(word.lower())\n",
    "        for word in tokens\n",
    "        if word.isalnum() and word.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "preprocessed_plots = [preprocess_text(plot) for plot in plots]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into TF-IDF vectors\n",
    "tfidf = tfidf_vectorizer.fit_transform(preprocessed_plots)\n",
    "\n",
    "\n",
    "def recomend_me(query, top_k=1):\n",
    "    movie_name = []\n",
    "    movie_plot = []\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "\n",
    "    tfidf_query = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "    cos_similarities = cosine_similarity(tfidf_query, tfidf)\n",
    "\n",
    "    sorted_idx = np.argsort(cos_similarities.squeeze())\n",
    "\n",
    "    for idx in reversed(sorted_idx[-top_k:]):\n",
    "        movie_name.append(df.loc[idx][\"Series_Title\"])\n",
    "        movie_plot.append(plots[idx])\n",
    "\n",
    "    return movie_name, list(reversed(cos_similarities[0, sorted_idx[-top_k:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = SimilarityRecommenderService()\n",
    "service.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Leonardo\"\n",
    "\n",
    "r1, c1 = service.recommend(query)\n",
    "m2, c2 = recomend_me(query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Revenant  (2015)', 'The Wolf of Wall Street  (2013)', 'Inception (2010)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Revenant', 4), ('The Wolf of Wall Street', 3), ('Inception', 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "for m, c in zip(r1, c1):\n",
    "    a = m.rsplit(\" \", 1)[0].strip()\n",
    "    L.append((a, c))\n",
    "\n",
    "\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Shutter Island', 'Titanic', 'The Departed'],\n",
       " [0.22160209763092573, 0.2113956480183037, 0.208104277409475])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "score = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for movie in L:\n",
    "    total += movie[1]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for movie in L:\n",
    "    score[movie[0]] = movie[1] / total\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Shutter Island': 0.2880827269202035,\n",
       "             'Titanic': 0.2748143424237948,\n",
       "             'The Departed': 0.27053556063231754})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for movie, score2 in zip(m2[0], m2[1]):\n",
    "    score[movie] += score2 * 1.3\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shutter Island', 0.2880827269202035),\n",
       " ('Titanic', 0.2748143424237948),\n",
       " ('The Departed', 0.27053556063231754)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_score = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shutter Island', 'Titanic', 'The Departed']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "recommended_movies = []\n",
    "for movie in sorted_score[:k]:\n",
    "    recommended_movies.append(movie[0])\n",
    "\n",
    "recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "date_expr = re.compile(r\"(\\([0-9]{4}\\))$\")\n",
    "\n",
    "\n",
    "def remove_date_from_movie(string: str):\n",
    "    return date_expr.sub(\"\", string).strip()\n",
    "\n",
    "\n",
    "def mixture_recommend(query):\n",
    "    r1, c1 = service.recommend(query, k=3)\n",
    "    r2, c2 = recomend_me(query, top_k=3)\n",
    "    L = []\n",
    "    for m, c in zip(r1, c1):\n",
    "        a = remove_date_from_movie(m)\n",
    "        L.append((a, c))\n",
    "    score = defaultdict(lambda: 0)\n",
    "    total = 0\n",
    "    for movie in L:\n",
    "        total += movie[1]\n",
    "    for movie in L:\n",
    "        score[movie[0]] = movie[1] / total\n",
    "    for movie, score2 in zip(r2, c2):\n",
    "        score[movie] += score2 * 1.5\n",
    "    sorted_score = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    k = 5\n",
    "    movies = []\n",
    "    confidence = 0\n",
    "    for m in sorted_score[:k]:\n",
    "        movies.append(m[0])\n",
    "        confidence += m[1]\n",
    "    return movies, confidence / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Black Swan', 'V for Vendetta', 'LÃ©on', 'American Made', 'Jackie'],\n",
       " 0.4735568372632484)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixture_recommend(\"Natalie Portman drama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
